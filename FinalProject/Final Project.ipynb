{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Career Exploration Final Project: TMDB Box Office Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.loc[train['id'] == 16,'revenue'] = 192864          # Skinning\n",
    "train.loc[train['id'] == 90,'budget'] = 30000000         # Sommersby          \n",
    "train.loc[train['id'] == 118,'budget'] = 60000000        # Wild Hogs\n",
    "train.loc[train['id'] == 149,'budget'] = 18000000        # Beethoven\n",
    "train.loc[train['id'] == 313,'revenue'] = 12000000       # The Cookout \n",
    "train.loc[train['id'] == 451,'revenue'] = 12000000       # Chasing Liberty\n",
    "train.loc[train['id'] == 464,'budget'] = 20000000        # Parenthood\n",
    "train.loc[train['id'] == 470,'budget'] = 13000000        # The Karate Kid, Part II\n",
    "train.loc[train['id'] == 513,'budget'] = 930000          # From Prada to Nada\n",
    "train.loc[train['id'] == 797,'budget'] = 8000000         # Welcome to Dongmakgol\n",
    "train.loc[train['id'] == 819,'budget'] = 90000000        # Alvin and the Chipmunks: The Road Chip\n",
    "train.loc[train['id'] == 850,'budget'] = 90000000        # Modern Times\n",
    "train.loc[train['id'] == 1007,'budget'] = 2              # Zyzzyx Road \n",
    "train.loc[train['id'] == 1112,'budget'] = 7500000        # An Officer and a Gentleman\n",
    "train.loc[train['id'] == 1131,'budget'] = 4300000        # Smokey and the Bandit   \n",
    "train.loc[train['id'] == 1359,'budget'] = 10000000       # Stir Crazy \n",
    "train.loc[train['id'] == 1542,'budget'] = 1              # All at Once\n",
    "train.loc[train['id'] == 1570,'budget'] = 15800000       # Crocodile Dundee II\n",
    "train.loc[train['id'] == 1571,'budget'] = 4000000        # Lady and the Tramp\n",
    "train.loc[train['id'] == 1714,'budget'] = 46000000       # The Recruit\n",
    "train.loc[train['id'] == 1721,'budget'] = 17500000       # Cocoon\n",
    "train.loc[train['id'] == 1865,'revenue'] = 25000000      # Scooby-Doo 2: Monsters Unleashed\n",
    "train.loc[train['id'] == 1885,'budget'] = 12             # In the Cut\n",
    "train.loc[train['id'] == 2091,'budget'] = 10             # Deadfall\n",
    "train.loc[train['id'] == 2268,'budget'] = 17500000       # Madea Goes to Jail budget\n",
    "train.loc[train['id'] == 2491,'budget'] = 6              # Never Talk to Strangers\n",
    "train.loc[train['id'] == 2602,'budget'] = 31000000       # Mr. Holland's Opus\n",
    "train.loc[train['id'] == 2612,'budget'] = 15000000       # Field of Dreams\n",
    "train.loc[train['id'] == 2696,'budget'] = 10000000       # Nurse 3-D\n",
    "train.loc[train['id'] == 2801,'budget'] = 10000000       # Fracture\n",
    "train.loc[train['id'] == 335,'budget'] = 2 \n",
    "train.loc[train['id'] == 348,'budget'] = 12\n",
    "train.loc[train['id'] == 470,'budget'] = 13000000 \n",
    "train.loc[train['id'] == 513,'budget'] = 1100000\n",
    "train.loc[train['id'] == 640,'budget'] = 6 \n",
    "train.loc[train['id'] == 696,'budget'] = 1\n",
    "train.loc[train['id'] == 797,'budget'] = 8000000 \n",
    "train.loc[train['id'] == 850,'budget'] = 1500000\n",
    "train.loc[train['id'] == 1199,'budget'] = 5 \n",
    "train.loc[train['id'] == 1282,'budget'] = 9               # Death at a Funeral\n",
    "train.loc[train['id'] == 1347,'budget'] = 1\n",
    "train.loc[train['id'] == 1755,'budget'] = 2\n",
    "train.loc[train['id'] == 1801,'budget'] = 5\n",
    "train.loc[train['id'] == 1918,'budget'] = 592 \n",
    "train.loc[train['id'] == 2033,'budget'] = 4\n",
    "train.loc[train['id'] == 2118,'budget'] = 344 \n",
    "train.loc[train['id'] == 2252,'budget'] = 130\n",
    "train.loc[train['id'] == 2256,'budget'] = 1 \n",
    "train.loc[train['id'] == 2696,'budget'] = 10000000\n",
    "\n",
    "#Clean Data\n",
    "test.loc[test['id'] == 6733,'budget'] = 5000000\n",
    "test.loc[test['id'] == 3889,'budget'] = 15000000\n",
    "test.loc[test['id'] == 6683,'budget'] = 50000000\n",
    "test.loc[test['id'] == 5704,'budget'] = 4300000\n",
    "test.loc[test['id'] == 6109,'budget'] = 281756\n",
    "test.loc[test['id'] == 7242,'budget'] = 10000000\n",
    "test.loc[test['id'] == 7021,'budget'] = 17540562       #  Two Is a Family\n",
    "test.loc[test['id'] == 5591,'budget'] = 4000000        # The Orphanage\n",
    "test.loc[test['id'] == 4282,'budget'] = 20000000       # Big Top Pee-wee\n",
    "test.loc[test['id'] == 3033,'budget'] = 250 \n",
    "test.loc[test['id'] == 3051,'budget'] = 50\n",
    "test.loc[test['id'] == 3084,'budget'] = 337\n",
    "test.loc[test['id'] == 3224,'budget'] = 4  \n",
    "test.loc[test['id'] == 3594,'budget'] = 25  \n",
    "test.loc[test['id'] == 3619,'budget'] = 500  \n",
    "test.loc[test['id'] == 3831,'budget'] = 3  \n",
    "test.loc[test['id'] == 3935,'budget'] = 500  \n",
    "test.loc[test['id'] == 4049,'budget'] = 995946 \n",
    "test.loc[test['id'] == 4424,'budget'] = 3  \n",
    "test.loc[test['id'] == 4460,'budget'] = 8  \n",
    "test.loc[test['id'] == 4555,'budget'] = 1200000 \n",
    "test.loc[test['id'] == 4624,'budget'] = 30 \n",
    "test.loc[test['id'] == 4645,'budget'] = 500 \n",
    "test.loc[test['id'] == 4709,'budget'] = 450 \n",
    "test.loc[test['id'] == 4839,'budget'] = 7\n",
    "test.loc[test['id'] == 3125,'budget'] = 25 \n",
    "test.loc[test['id'] == 3142,'budget'] = 1\n",
    "test.loc[test['id'] == 3201,'budget'] = 450\n",
    "test.loc[test['id'] == 3222,'budget'] = 6\n",
    "test.loc[test['id'] == 3545,'budget'] = 38\n",
    "test.loc[test['id'] == 3670,'budget'] = 18\n",
    "test.loc[test['id'] == 3792,'budget'] = 19\n",
    "test.loc[test['id'] == 3881,'budget'] = 7\n",
    "test.loc[test['id'] == 3969,'budget'] = 400\n",
    "test.loc[test['id'] == 4196,'budget'] = 6\n",
    "test.loc[test['id'] == 4221,'budget'] = 11\n",
    "test.loc[test['id'] == 4222,'budget'] = 500\n",
    "test.loc[test['id'] == 4285,'budget'] = 11\n",
    "test.loc[test['id'] == 4319,'budget'] = 1\n",
    "test.loc[test['id'] == 4639,'budget'] = 10\n",
    "test.loc[test['id'] == 4719,'budget'] = 45\n",
    "test.loc[test['id'] == 4822,'budget'] = 22\n",
    "test.loc[test['id'] == 4829,'budget'] = 20\n",
    "test.loc[test['id'] == 4969,'budget'] = 20\n",
    "test.loc[test['id'] == 5021,'budget'] = 40 \n",
    "test.loc[test['id'] == 5035,'budget'] = 1 \n",
    "test.loc[test['id'] == 5063,'budget'] = 14 \n",
    "test.loc[test['id'] == 5119,'budget'] = 2 \n",
    "test.loc[test['id'] == 5214,'budget'] = 30 \n",
    "test.loc[test['id'] == 5221,'budget'] = 50 \n",
    "test.loc[test['id'] == 4903,'budget'] = 15\n",
    "test.loc[test['id'] == 4983,'budget'] = 3\n",
    "test.loc[test['id'] == 5102,'budget'] = 28\n",
    "test.loc[test['id'] == 5217,'budget'] = 75\n",
    "test.loc[test['id'] == 5224,'budget'] = 3 \n",
    "test.loc[test['id'] == 5469,'budget'] = 20 \n",
    "test.loc[test['id'] == 5840,'budget'] = 1 \n",
    "test.loc[test['id'] == 5960,'budget'] = 30\n",
    "test.loc[test['id'] == 6506,'budget'] = 11 \n",
    "test.loc[test['id'] == 6553,'budget'] = 280\n",
    "test.loc[test['id'] == 6561,'budget'] = 7\n",
    "test.loc[test['id'] == 6582,'budget'] = 218\n",
    "test.loc[test['id'] == 6638,'budget'] = 5\n",
    "test.loc[test['id'] == 6749,'budget'] = 8 \n",
    "test.loc[test['id'] == 6759,'budget'] = 50 \n",
    "test.loc[test['id'] == 6856,'budget'] = 10\n",
    "test.loc[test['id'] == 6858,'budget'] =  100\n",
    "test.loc[test['id'] == 6876,'budget'] =  250\n",
    "test.loc[test['id'] == 6972,'budget'] = 1\n",
    "test.loc[test['id'] == 7079,'budget'] = 8000000\n",
    "test.loc[test['id'] == 7150,'budget'] = 118\n",
    "test.loc[test['id'] == 6506,'budget'] = 118\n",
    "test.loc[test['id'] == 7225,'budget'] = 6\n",
    "test.loc[test['id'] == 7231,'budget'] = 85\n",
    "test.loc[test['id'] == 5222,'budget'] = 5\n",
    "test.loc[test['id'] == 5322,'budget'] = 90\n",
    "test.loc[test['id'] == 5350,'budget'] = 70\n",
    "test.loc[test['id'] == 5378,'budget'] = 10\n",
    "test.loc[test['id'] == 5545,'budget'] = 80\n",
    "test.loc[test['id'] == 5810,'budget'] = 8\n",
    "test.loc[test['id'] == 5926,'budget'] = 300\n",
    "test.loc[test['id'] == 5927,'budget'] = 4\n",
    "test.loc[test['id'] == 5986,'budget'] = 1\n",
    "test.loc[test['id'] == 6053,'budget'] = 20\n",
    "test.loc[test['id'] == 6104,'budget'] = 1\n",
    "test.loc[test['id'] == 6130,'budget'] = 30\n",
    "test.loc[test['id'] == 6301,'budget'] = 150\n",
    "test.loc[test['id'] == 6276,'budget'] = 100\n",
    "test.loc[test['id'] == 6473,'budget'] = 100\n",
    "test.loc[test['id'] == 6842,'budget'] = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train = train.drop('revenue', axis=1), train['revenue']\n",
    "X_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat((X_train, X_test), axis=0)#, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def empty_listify(lst):\n",
    "    return [] if pd.isnull(lst) else eval(lst)\n",
    "\n",
    "def json_to_list_of_ids(col):\n",
    "    return pd.Series(\n",
    "        map(\n",
    "            lambda lst: list(col['id'] for col in empty_listify(lst)), \n",
    "            col\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from collections import defaultdict\n",
    "\n",
    "def pcaify(one_hot, column_prefix, num_pca_columns):\n",
    "    pca = PCA(n_components=num_pca_columns)    \n",
    "    features = pca.fit_transform(one_hot)\n",
    "    \n",
    "    return pd.DataFrame(data = features, columns = ['{0}_{1}'.format(column_prefix, i) for i in range(features.shape[1])])\n",
    "\n",
    "def one_hot_encode_json_col(col, prefix, num_columns, num_pca_columns):\n",
    "    dummies = []\n",
    "\n",
    "    counts = defaultdict(int)\n",
    "    \n",
    "    for entry in col:\n",
    "        for elem in empty_listify(entry):\n",
    "            counts[elem['name']] += 1\n",
    "    \n",
    "    top = {k for k, v in sorted(counts.items(), key=lambda item: -counts[item[0]])[:num_columns]}\n",
    "    \n",
    "    for entry in col:\n",
    "        d = {}\n",
    "        for elem in empty_listify(entry):\n",
    "            if elem['name'] in top:\n",
    "                d[f'{prefix}_{elem[\"name\"]}'] = 1\n",
    "        dummies.append(d)\n",
    "    \n",
    "    if len(top) < num_pca_columns:\n",
    "        return pd.DataFrame(dummies).fillna(0).set_index(df.index)\n",
    "    \n",
    "    one_hot = pd.DataFrame(dummies).fillna(0).values\n",
    "    \n",
    "    return pcaify(one_hot, prefix, num_pca_columns).set_index(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat((df, one_hot_encode_json_col(df['genres'], 'genre', num_columns=100, num_pca_columns=30)), axis=1).drop('genres', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat((df, one_hot_encode_json_col(df['production_companies'], 'pc', num_columns=100, num_pca_columns=10)), axis=1).drop('production_companies', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat((df, one_hot_encode_json_col(df['Keywords'], 'kw', num_columns=1000, num_pca_columns=50)), axis=1).drop('Keywords', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(sentence) \n",
    "    return ' '.join([w for w in word_tokens if not w in stop_words])\n",
    "\n",
    "def get_text_features(df, text_col, num_text_features=10):\n",
    "    print('Removing stopwords')\n",
    "    text = df[text_col].fillna('').apply(remove_stopwords)\n",
    "\n",
    "    print('Creating bag of words')\n",
    "    vectorizer = HashingVectorizer()\n",
    "    text_bow = vectorizer.fit_transform(text).toarray()\n",
    "    \n",
    "    print('Running PCA')\n",
    "    pca = PCA(n_components=num_text_features)\n",
    "    text_features = pca.fit_transform(text_bow)\n",
    "    \n",
    "    return pd.DataFrame(data = text_features, columns = ['{0}_{1}'.format(text_col, i) for i in range(text_features.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_data = df[['overview', 'tagline', 'original_title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['has_tagline'] = df['tagline'].isnull().astype(int)\n",
    "df = df.drop('tagline', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Release Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "release_date = pd.to_datetime(df['release_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "release_date[pd.isnull(release_date)] = pd.to_datetime((release_date - release_date.min()).mean() + release_date.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['release_date_days'] = (release_date - release_date.min()).apply(lambda d: d.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['release_date_month'] = release_date.apply(lambda d: d.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['release_date_year'] = release_date.apply(lambda d: d.year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjusting for year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df['meanruntimeByYear'] = df.groupby(\"release_date_year\")[\"runtime\"].aggregate('mean')\n",
    "# df['meanPopularityByYear'] = df.groupby(\"release_date_year\")[\"popularity\"].aggregate('mean')\n",
    "# df['meanBudgetByYear'] = df.groupby(\"release_date_year\")[\"budget\"].aggregate('mean')\n",
    "# df['medianBudgetByYear'] = df.groupby(\"release_date_year\")[\"budget\"].aggregate('median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop('release_date', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cast = df['cast']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat((df, one_hot_encode_json_col(cast, 'cast', num_columns=1000, num_pca_columns=100)), axis=1).drop('cast', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cast_male_counts = []\n",
    "cast_female_counts = []\n",
    "\n",
    "for entry in cast:\n",
    "    entry_data = empty_listify(entry)\n",
    "    if entry_data:\n",
    "        cast_male_counts.append(\n",
    "            sum(1 for e in entry_data if e['gender'] == 2)\n",
    "        )\n",
    "        cast_female_counts.append(\n",
    "            sum(1 for e in entry_data if e['gender'] == 1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['num_male'] = pd.Series(cast_male_counts)\n",
    "df['num_female'] = pd.Series(cast_female_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crew = df['crew']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crew_counts = []\n",
    "\n",
    "for entry in crew:\n",
    "    entry_data = empty_listify(entry)\n",
    "    if entry_data:\n",
    "        crew_counts.append(len(entry_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['num_crew'] = pd.Series(crew_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop('crew', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat((df, pcaify(pd.get_dummies(df['original_language']), 'og_lang', 20).set_index(df.index)), axis=1)\n",
    "df = df.drop('original_language', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spoken Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat((df, one_hot_encode_json_col(df['spoken_languages'], 'sl', num_columns=500, num_pca_columns=10)), axis=1).drop('spoken_languages', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Homepage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['has_homepage'] = df['homepage'].isnull().astype(int)\n",
    "df = df.drop('homepage', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Production Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat((df, one_hot_encode_json_col(df['production_countries'], 'sl', num_columns=500, num_pca_columns=10)), axis=1).drop('production_countries', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['is_released'] = (df['status'] == 'Released').astype(int)\n",
    "df = df.drop('status', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(['status', 'title', 'id', 'belongs_to_collection', 'original_title', 'imdb_id', 'overview', 'poster_path', 'original_title'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['runtime'] = df['runtime'].fillna(df['runtime'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.DataFrame(data = StandardScaler().fit_transform(df), columns = df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df#[['budget', 'popularity', 'runtime', 'release_date_days', 'release_date_month', 'has_homepage', 'has_tagline']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "def evaluate(y_pred, y_true):\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train = X[:train.shape[0]]\n",
    "y_train = y_train\n",
    "X_test = X[train.shape[0]:]\n",
    "\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(model, X):\n",
    "    non_transformed = model.predict(X)\n",
    "    non_transformed = np.clip(non_transformed, 0, non_transformed.max())\n",
    "    return np.exp(non_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Lasso(alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.05, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=None,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X, np.log(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.231733871433978"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(predict(model, valid_X), valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.329478029220208"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(predict(model, train_X), train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajay/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(max_depth=8, random_state=8, n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=8,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "           oob_score=False, random_state=8, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X, np.log(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4663611741259006"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(predict(model, train_X), train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.95752059850231"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(predict(model, valid_X), valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "model = KNeighborsRegressor(n_neighbors=20)\n",
    "model.fit(train_X, np.log(train_y))\n",
    "y_pred = model.predict(valid_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.092278932332581"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(predict(model, valid_X), valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(train_X.shape[1], activation='relu', input_shape=(train_X.shape[1],)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu', input_shape=(64,)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu', input_shape=(32,)))\n",
    "model.add(Dense(1, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "model.compile(optimizer = \"sgd\", loss = root_mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2400/2400 [==============================] - 0s 201us/step - loss: 10.5744\n",
      "Epoch 2/200\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 6.0602\n",
      "Epoch 3/200\n",
      "2400/2400 [==============================] - 0s 64us/step - loss: 5.2016\n",
      "Epoch 4/200\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 4.8252\n",
      "Epoch 5/200\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 4.4466\n",
      "Epoch 6/200\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 4.1408\n",
      "Epoch 7/200\n",
      "2400/2400 [==============================] - 0s 63us/step - loss: 4.0487\n",
      "Epoch 8/200\n",
      "2400/2400 [==============================] - 0s 66us/step - loss: 3.8179: 0s - loss: 3.825\n",
      "Epoch 9/200\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 3.6143\n",
      "Epoch 10/200\n",
      "2400/2400 [==============================] - 0s 65us/step - loss: 3.4879\n",
      "Epoch 11/200\n",
      "2400/2400 [==============================] - 0s 64us/step - loss: 3.3094\n",
      "Epoch 12/200\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 3.1612\n",
      "Epoch 13/200\n",
      "2400/2400 [==============================] - 0s 73us/step - loss: 3.0088\n",
      "Epoch 14/200\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 2.9343\n",
      "Epoch 15/200\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 2.9366\n",
      "Epoch 16/200\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 2.7867\n",
      "Epoch 17/200\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 2.7366\n",
      "Epoch 18/200\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 2.6721\n",
      "Epoch 19/200\n",
      "2400/2400 [==============================] - 0s 77us/step - loss: 2.6576\n",
      "Epoch 20/200\n",
      "2400/2400 [==============================] - 0s 72us/step - loss: 2.6179\n",
      "Epoch 21/200\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 2.6022\n",
      "Epoch 22/200\n",
      "2400/2400 [==============================] - 0s 78us/step - loss: 2.5581\n",
      "Epoch 23/200\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 2.5344\n",
      "Epoch 24/200\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 2.5393\n",
      "Epoch 25/200\n",
      "2400/2400 [==============================] - 0s 73us/step - loss: 2.5037\n",
      "Epoch 26/200\n",
      "2400/2400 [==============================] - 0s 71us/step - loss: 2.5001\n",
      "Epoch 27/200\n",
      "2400/2400 [==============================] - 0s 72us/step - loss: 2.5041\n",
      "Epoch 28/200\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 2.5048\n",
      "Epoch 29/200\n",
      "2400/2400 [==============================] - 0s 63us/step - loss: 2.4553\n",
      "Epoch 30/200\n",
      "2400/2400 [==============================] - 0s 65us/step - loss: 2.4617\n",
      "Epoch 31/200\n",
      "2400/2400 [==============================] - 0s 71us/step - loss: 2.4352\n",
      "Epoch 32/200\n",
      "2400/2400 [==============================] - 0s 78us/step - loss: 2.3888\n",
      "Epoch 33/200\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 2.4360\n",
      "Epoch 34/200\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 2.4032\n",
      "Epoch 35/200\n",
      "2400/2400 [==============================] - 0s 66us/step - loss: 2.4568\n",
      "Epoch 36/200\n",
      "2400/2400 [==============================] - 0s 65us/step - loss: 2.4020\n",
      "Epoch 37/200\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 2.3775\n",
      "Epoch 38/200\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 2.3799\n",
      "Epoch 39/200\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 2.3671\n",
      "Epoch 40/200\n",
      "2400/2400 [==============================] - 0s 64us/step - loss: 2.4076\n",
      "Epoch 41/200\n",
      "2400/2400 [==============================] - 0s 72us/step - loss: 2.3948\n",
      "Epoch 42/200\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 2.3551\n",
      "Epoch 43/200\n",
      "2400/2400 [==============================] - 0s 73us/step - loss: 2.3867\n",
      "Epoch 44/200\n",
      "2400/2400 [==============================] - 0s 73us/step - loss: 2.3396\n",
      "Epoch 45/200\n",
      "2400/2400 [==============================] - 0s 65us/step - loss: 2.3574\n",
      "Epoch 46/200\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 2.3804\n",
      "Epoch 47/200\n",
      "2400/2400 [==============================] - 0s 65us/step - loss: 2.3181\n",
      "Epoch 48/200\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 2.3527\n",
      "Epoch 49/200\n",
      "2400/2400 [==============================] - 0s 71us/step - loss: 2.3328\n",
      "Epoch 50/200\n",
      "2400/2400 [==============================] - 0s 77us/step - loss: 2.3326\n",
      "Epoch 51/200\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 2.3095\n",
      "Epoch 52/200\n",
      "2400/2400 [==============================] - 0s 61us/step - loss: 2.3094\n",
      "Epoch 53/200\n",
      "2400/2400 [==============================] - 0s 61us/step - loss: 2.3137\n",
      "Epoch 54/200\n",
      "2400/2400 [==============================] - 0s 60us/step - loss: 2.3133\n",
      "Epoch 55/200\n",
      "2400/2400 [==============================] - 0s 73us/step - loss: 2.2913\n",
      "Epoch 56/200\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 2.3128\n",
      "Epoch 57/200\n",
      "2400/2400 [==============================] - 0s 63us/step - loss: 2.2955\n",
      "Epoch 58/200\n",
      "2400/2400 [==============================] - 0s 65us/step - loss: 2.2971\n",
      "Epoch 59/200\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 2.3100\n",
      "Epoch 60/200\n",
      "2400/2400 [==============================] - 0s 79us/step - loss: 2.2880\n",
      "Epoch 61/200\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 2.2566\n",
      "Epoch 62/200\n",
      "2400/2400 [==============================] - 0s 84us/step - loss: 2.2668\n",
      "Epoch 63/200\n",
      "2400/2400 [==============================] - 0s 80us/step - loss: 2.2550\n",
      "Epoch 64/200\n",
      "2400/2400 [==============================] - 0s 73us/step - loss: 2.2298\n",
      "Epoch 65/200\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 2.2422\n",
      "Epoch 66/200\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 2.2353\n",
      "Epoch 67/200\n",
      "2400/2400 [==============================] - 0s 71us/step - loss: 2.2588\n",
      "Epoch 68/200\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 2.2520\n",
      "Epoch 69/200\n",
      "2400/2400 [==============================] - 0s 71us/step - loss: 2.2278\n",
      "Epoch 70/200\n",
      "2400/2400 [==============================] - 0s 66us/step - loss: 2.2724\n",
      "Epoch 71/200\n",
      "2400/2400 [==============================] - 0s 64us/step - loss: 2.2258\n",
      "Epoch 72/200\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 2.2287\n",
      "Epoch 73/200\n",
      "2400/2400 [==============================] - 0s 66us/step - loss: 2.1966\n",
      "Epoch 74/200\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 2.2068\n",
      "Epoch 75/200\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 2.2071\n",
      "Epoch 76/200\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 2.1781\n",
      "Epoch 77/200\n",
      "2400/2400 [==============================] - 0s 63us/step - loss: 2.1937\n",
      "Epoch 78/200\n",
      "2400/2400 [==============================] - 0s 63us/step - loss: 2.1712\n",
      "Epoch 79/200\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 2.1954\n",
      "Epoch 80/200\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 2.1732\n",
      "Epoch 81/200\n",
      "2400/2400 [==============================] - 0s 64us/step - loss: 2.1840\n",
      "Epoch 82/200\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 2.1912\n",
      "Epoch 83/200\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 2.1907\n",
      "Epoch 84/200\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 2.2191\n",
      "Epoch 85/200\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 2.1455\n",
      "Epoch 86/200\n",
      "2400/2400 [==============================] - 0s 62us/step - loss: 2.1768\n",
      "Epoch 87/200\n",
      "2400/2400 [==============================] - 0s 64us/step - loss: 2.1524\n",
      "Epoch 88/200\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 2.1950\n",
      "Epoch 89/200\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 2.1662\n",
      "Epoch 90/200\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 2.1551\n",
      "Epoch 91/200\n",
      "2400/2400 [==============================] - 0s 63us/step - loss: 2.1495\n",
      "Epoch 92/200\n",
      "2400/2400 [==============================] - 0s 65us/step - loss: 2.1364\n",
      "Epoch 93/200\n",
      "2400/2400 [==============================] - 0s 71us/step - loss: 2.1192\n",
      "Epoch 94/200\n",
      "2400/2400 [==============================] - 0s 71us/step - loss: 2.1211\n",
      "Epoch 95/200\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 2.1085\n",
      "Epoch 96/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2400 [==============================] - 0s 67us/step - loss: 2.1438\n",
      "Epoch 97/200\n",
      "2400/2400 [==============================] - 0s 64us/step - loss: 2.1127\n",
      "Epoch 98/200\n",
      "2400/2400 [==============================] - 0s 64us/step - loss: 2.0886\n",
      "Epoch 99/200\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 2.0810\n",
      "Epoch 100/200\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 2.0585\n",
      "Epoch 101/200\n",
      "2400/2400 [==============================] - 0s 71us/step - loss: 2.0543\n",
      "Epoch 102/200\n",
      "2400/2400 [==============================] - 0s 63us/step - loss: 2.0115\n",
      "Epoch 103/200\n",
      "2400/2400 [==============================] - 0s 65us/step - loss: 1.9837\n",
      "Epoch 104/200\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 1.9949\n",
      "Epoch 105/200\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 1.9964\n",
      "Epoch 106/200\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 2.0199\n",
      "Epoch 107/200\n",
      "2400/2400 [==============================] - 0s 65us/step - loss: 2.0212\n",
      "Epoch 108/200\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 2.0039\n",
      "Epoch 109/200\n",
      "2400/2400 [==============================] - 0s 66us/step - loss: 1.9664\n",
      "Epoch 110/200\n",
      "2400/2400 [==============================] - 0s 64us/step - loss: 2.0000\n",
      "Epoch 111/200\n",
      "2400/2400 [==============================] - 0s 66us/step - loss: 1.9685\n",
      "Epoch 112/200\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 1.9358\n",
      "Epoch 113/200\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 1.9325\n",
      "Epoch 114/200\n",
      "2400/2400 [==============================] - 0s 71us/step - loss: 1.9765\n",
      "Epoch 115/200\n",
      "2400/2400 [==============================] - 0s 72us/step - loss: 1.9373\n",
      "Epoch 116/200\n",
      "2400/2400 [==============================] - 0s 66us/step - loss: 1.9011\n",
      "Epoch 117/200\n",
      "2400/2400 [==============================] - 0s 65us/step - loss: 1.9042\n",
      "Epoch 118/200\n",
      "2400/2400 [==============================] - 0s 71us/step - loss: 1.9141\n",
      "Epoch 119/200\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 1.8781\n",
      "Epoch 120/200\n",
      "2400/2400 [==============================] - 0s 66us/step - loss: 1.8856\n",
      "Epoch 121/200\n",
      "2400/2400 [==============================] - 0s 65us/step - loss: 1.8949\n",
      "Epoch 122/200\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 1.8427\n",
      "Epoch 123/200\n",
      "2400/2400 [==============================] - 0s 72us/step - loss: 1.8323\n",
      "Epoch 124/200\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 1.8458\n",
      "Epoch 125/200\n",
      "2400/2400 [==============================] - 0s 72us/step - loss: 1.8107\n",
      "Epoch 126/200\n",
      "2400/2400 [==============================] - 0s 64us/step - loss: 1.7978\n",
      "Epoch 127/200\n",
      "2400/2400 [==============================] - 0s 65us/step - loss: 1.8078\n",
      "Epoch 128/200\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 1.8280\n",
      "Epoch 129/200\n",
      "2400/2400 [==============================] - 0s 71us/step - loss: 1.7666\n",
      "Epoch 130/200\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 1.7770\n",
      "Epoch 131/200\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 1.7536\n",
      "Epoch 132/200\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 1.7600\n",
      "Epoch 133/200\n",
      "2400/2400 [==============================] - 0s 66us/step - loss: 1.7911\n",
      "Epoch 134/200\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 1.7625\n",
      "Epoch 135/200\n",
      "2400/2400 [==============================] - 0s 71us/step - loss: 1.7537\n",
      "Epoch 136/200\n",
      "2400/2400 [==============================] - 0s 72us/step - loss: 1.7226\n",
      "Epoch 137/200\n",
      "2400/2400 [==============================] - 0s 72us/step - loss: 1.7112\n",
      "Epoch 138/200\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 1.7425\n",
      "Epoch 139/200\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 1.6752\n",
      "Epoch 140/200\n",
      "2400/2400 [==============================] - 0s 66us/step - loss: 1.7351\n",
      "Epoch 141/200\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 1.6659\n",
      "Epoch 142/200\n",
      "2400/2400 [==============================] - 0s 72us/step - loss: 1.6782\n",
      "Epoch 143/200\n",
      "2400/2400 [==============================] - 0s 72us/step - loss: 1.6311\n",
      "Epoch 144/200\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 1.6973\n",
      "Epoch 145/200\n",
      "2400/2400 [==============================] - 0s 66us/step - loss: 1.6399\n",
      "Epoch 146/200\n",
      "2400/2400 [==============================] - 0s 64us/step - loss: 1.6252\n",
      "Epoch 147/200\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 1.6783\n",
      "Epoch 148/200\n",
      "2400/2400 [==============================] - 0s 65us/step - loss: 1.6751\n",
      "Epoch 149/200\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 1.6887\n",
      "Epoch 150/200\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 1.6765\n",
      "Epoch 151/200\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 1.6276\n",
      "Epoch 152/200\n",
      "2400/2400 [==============================] - 0s 64us/step - loss: 1.6315\n",
      "Epoch 153/200\n",
      "2400/2400 [==============================] - 0s 66us/step - loss: 1.6398\n",
      "Epoch 154/200\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 1.6397\n",
      "Epoch 155/200\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 1.6148\n",
      "Epoch 156/200\n",
      "2400/2400 [==============================] - 0s 72us/step - loss: 1.5957\n",
      "Epoch 157/200\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 1.6345\n",
      "Epoch 158/200\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 1.6054\n",
      "Epoch 159/200\n",
      "2400/2400 [==============================] - 0s 71us/step - loss: 1.6555\n",
      "Epoch 160/200\n",
      "2400/2400 [==============================] - 0s 72us/step - loss: 1.6004\n",
      "Epoch 161/200\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 1.6202\n",
      "Epoch 162/200\n",
      "2400/2400 [==============================] - 0s 71us/step - loss: 1.5752\n",
      "Epoch 163/200\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 1.6082\n",
      "Epoch 164/200\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 1.6203\n",
      "Epoch 165/200\n",
      "2400/2400 [==============================] - 0s 72us/step - loss: 1.5959\n",
      "Epoch 166/200\n",
      "2400/2400 [==============================] - 0s 72us/step - loss: 1.5733\n",
      "Epoch 167/200\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 1.5806\n",
      "Epoch 168/200\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 1.5900\n",
      "Epoch 169/200\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 1.5816\n",
      "Epoch 170/200\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 1.5655\n",
      "Epoch 171/200\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 1.5525\n",
      "Epoch 172/200\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 1.5532\n",
      "Epoch 173/200\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 1.6085\n",
      "Epoch 174/200\n",
      "2400/2400 [==============================] - 0s 73us/step - loss: 1.5579\n",
      "Epoch 175/200\n",
      "2400/2400 [==============================] - 0s 71us/step - loss: 1.5590\n",
      "Epoch 176/200\n",
      "2400/2400 [==============================] - 0s 71us/step - loss: 1.4933\n",
      "Epoch 177/200\n",
      "2400/2400 [==============================] - 0s 71us/step - loss: 1.5457\n",
      "Epoch 178/200\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 1.5018\n",
      "Epoch 179/200\n",
      "2400/2400 [==============================] - 0s 71us/step - loss: 1.5761\n",
      "Epoch 180/200\n",
      "2400/2400 [==============================] - 0s 71us/step - loss: 1.5189\n",
      "Epoch 181/200\n",
      "2400/2400 [==============================] - 0s 65us/step - loss: 1.5415\n",
      "Epoch 182/200\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 1.5681\n",
      "Epoch 183/200\n",
      "2400/2400 [==============================] - 0s 72us/step - loss: 1.5081\n",
      "Epoch 184/200\n",
      "2400/2400 [==============================] - 0s 72us/step - loss: 1.5380\n",
      "Epoch 185/200\n",
      "2400/2400 [==============================] - 0s 72us/step - loss: 1.5645\n",
      "Epoch 186/200\n",
      "2400/2400 [==============================] - 0s 71us/step - loss: 1.5004\n",
      "Epoch 187/200\n",
      "2400/2400 [==============================] - 0s 71us/step - loss: 1.5066\n",
      "Epoch 188/200\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 1.4892\n",
      "Epoch 189/200\n",
      "2400/2400 [==============================] - 0s 73us/step - loss: 1.4936\n",
      "Epoch 190/200\n",
      "2400/2400 [==============================] - 0s 72us/step - loss: 1.5123\n",
      "Epoch 191/200\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 1.5135\n",
      "Epoch 192/200\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 1.4972\n",
      "Epoch 193/200\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 1.4643\n",
      "Epoch 194/200\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 1.5003\n",
      "Epoch 195/200\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 1.4771\n",
      "Epoch 196/200\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 1.4858\n",
      "Epoch 197/200\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 1.4934\n",
      "Epoch 198/200\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 1.4868\n",
      "Epoch 199/200\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 1.4857\n",
      "Epoch 200/200\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 1.4469\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a30d99ba8>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=train_X.values, y=np.log(train_y.values), epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.361275334834239"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(predict(model, valid_X.values), valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7037621572063717"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(predict(model, train_X.values), train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "random_seed = 42\n",
    "k = 10\n",
    "fold = list(KFold(k, shuffle = True, random_state = random_seed).split(X_train.values))\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from datetime import datetime\n",
    "\n",
    "def xgb_model(trn_x, trn_y, val_x, val_y, test, verbose) :\n",
    "    \n",
    "    params = {'objective': 'reg:linear', \n",
    "              'eta': 0.01, \n",
    "              'max_depth': 6, \n",
    "              'subsample': 0.6, \n",
    "              'colsample_bytree': 0.7,  \n",
    "              'eval_metric': 'rmse', \n",
    "              'seed': random_seed, \n",
    "              'silent': True,\n",
    "    }\n",
    "    \n",
    "    record = dict()\n",
    "    model = xgb.train(params\n",
    "                      , xgb.DMatrix(trn_x, trn_y)\n",
    "                      , 100000\n",
    "                      , [(xgb.DMatrix(trn_x, trn_y), 'train'), (xgb.DMatrix(val_x, val_y), 'valid')]\n",
    "                      , verbose_eval=verbose\n",
    "                      , early_stopping_rounds=500\n",
    "                      , callbacks = [xgb.callback.record_evaluation(record)])\n",
    "    best_idx = np.argmin(np.array(record['valid']['rmse']))\n",
    "\n",
    "    val_pred = model.predict(xgb.DMatrix(val_x), ntree_limit=model.best_ntree_limit)\n",
    "    test_pred = model.predict(xgb.DMatrix(X_test.values), ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "    return {'val':val_pred, 'test':test_pred, 'error':record['valid']['rmse'][best_idx], 'importance':[i for k, i in model.get_score().items()]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_xgb(X_train, y_train, X_test):\n",
    "    result_dict = dict()\n",
    "    val_pred = np.zeros(X_train.values.shape[0])\n",
    "    test_pred = np.zeros(X_test.shape[0])\n",
    "    final_err = 0\n",
    "    verbose = False\n",
    "\n",
    "    for i, (trn, val) in enumerate(fold) :\n",
    "        print(i+1, \"fold.    RMSE\")\n",
    "\n",
    "        trn_x = X_train.values[trn, :]\n",
    "        trn_y = y_train[trn]\n",
    "        val_x = X_train.values[val, :]\n",
    "        val_y = y_train[val]\n",
    "\n",
    "        fold_val_pred = []\n",
    "        fold_test_pred = []\n",
    "        fold_err = []\n",
    "\n",
    "        #\"\"\" xgboost\n",
    "        start = datetime.now()\n",
    "        result = xgb_model(trn_x, trn_y, val_x, val_y, X_test, verbose)\n",
    "        fold_val_pred.append(result['val'])\n",
    "        fold_test_pred.append(result['test'])\n",
    "        fold_err.append(result['error'])\n",
    "        print(\"xgb model.\", \"{0:.5f}\".format(result['error']), '(' + str(int((datetime.now()-start).seconds/60)) + 'm)')\n",
    "        #\"\"\"\n",
    "\n",
    "        val_pred[val] += np.mean(np.array(fold_val_pred), axis = 0)\n",
    "        test_pred += np.mean(np.array(fold_test_pred), axis = 0) / k\n",
    "        final_err += (sum(fold_err) / len(fold_err)) / k\n",
    "\n",
    "    print(\"fianl avg   err.\", final_err)\n",
    "    print(\"fianl blend err.\", np.sqrt(np.mean((val_pred - y_train)**2)))\n",
    "    \n",
    "    return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 fold.    RMSE\n",
      "xgb model. 1.91630 (0m)\n",
      "2 fold.    RMSE\n",
      "xgb model. 1.94071 (0m)\n",
      "3 fold.    RMSE\n",
      "xgb model. 2.04864 (0m)\n",
      "4 fold.    RMSE\n",
      "xgb model. 1.92870 (0m)\n",
      "5 fold.    RMSE\n",
      "xgb model. 2.17795 (0m)\n",
      "6 fold.    RMSE\n",
      "xgb model. 1.85512 (0m)\n",
      "7 fold.    RMSE\n",
      "xgb model. 2.33851 (0m)\n",
      "8 fold.    RMSE\n",
      "xgb model. 2.27653 (2m)\n",
      "9 fold.    RMSE\n",
      "xgb model. 2.24241 (0m)\n",
      "10 fold.    RMSE\n",
      "xgb model. 2.05955 (0m)\n",
      "fianl avg   err. 2.0784402\n",
      "fianl blend err. 2.0847448735154424\n"
     ]
    }
   ],
   "source": [
    "xgb_preds = run_xgb(X_train, np.log(y_train), X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preds = predict(model, X_test).reshape(-1)\n",
    "preds = np.exp(xgb_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out = pd.DataFrame(data={'id': test['id'], 'revenue': preds}).set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out.to_csv('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deeplearning)",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
